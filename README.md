# Open Insider Trades API

## Description
This project provides API endpoints to retrieve insider trading transactions (SEC Form 4 Insider Trading activities) based on the stock ticker, trade type, and date range. The data is scraped from [openinsider.com](http://openinsider.com) using BeautifulSoup and is stored in an SQLite database using SQLAlchemy. This project was created as a personal hobby project to integrate with my own tools, as I couldnâ€™t find a suitable source for insider trades. Additional features may be added in the future.

**Technologies Used:**
- FastAPI â€“ for building the API.
- BeautifulSoup â€“ for web scraping from openinsider.com.
- SQLAlchemy â€“ for ORM-based database interactions.
- SQLite3 â€“ as the primary database.
- Redis â€“ for rate limiting and caching. 
- [JWT Authentication](https://datatracker.ietf.org/doc/html/rfc7519) with refresh_token

## Features

### Basic Features
| Feature                         | Status | Notes                                                                 |
|---------------------------------|--------|-----------------------------------------------------------------------|
| **JWT Authentication**          | âœ…     | Access token in `Authorization` header; refresh token in cookie      |
| **Refresh Token in Cookie**     | âœ…     | Stored in `HttpOnly` cookie to prevent JavaScript access             |
| **CSRF Protection**             | âœ…     | CSRF token in cookie + `X-CSRF-Token` header                |
| **Rate Limiting (per client)**  | âœ…     | Sliding window; Redis-backed with SQLite fallback                    |
| **Custom Error Handling**       | âœ…     | Clean JSON error responses with consistent structure                 |

---

### User Endpoints (Login Required)

 **Docs**  
- Swagger / OpenAPI: `{{base-url}}/docs#/`
- Redoc: `{{base-url}}/redoc`

| Feature                                              | Status | Endpoint / Notes                                                                 |
|-------------------------------------------------------|--------|-----------------------------------------------------------------------------|
| **Get insider trades by ticker**               | âœ…     | `GET /insider_trades/{{ticker_id}}`  - Retrieve insider trades by stock ticker. Optional: date range, type        |
| **Get insider trades by date range**           | âœ…     | `GET /insider_trades`  - Retrieve insider trades by date range. Optional: transaction type          |

---

### Admin Features (Login Required)

| Feature                        | Status | Endpoint / Notes                                                                 |
|--------------------------------|--------|----------------------------------------------------------------------------------|
| **Force Refresh / Bootstrapping** | âœ…     | `POST /admin/bootstrap` - Run scraping script to refresh data                  |
| **Generate Client ID**         | âœ…     | `POST /admin/generate_client_id` - Returns a one-time-use ID and password      |
| **Daily Sync**                 | âœ…     | Enabled by default - runs at midnight UTC (not exposed as an endpoint)         |
| **Enable/Disable Daily Sync** | ðŸš§     | Planned - toggle daily sync task (Celery + Redis)                               |
| **Dump Data to CSV**           | ðŸš§     | Planned - export DB contents to CSV                                            |
| **Bulk Upload (CSV)**          | ðŸš§     | Planned - file limit ~5MB                                                      |
| &nbsp;&nbsp;&nbsp;â€¢ Validate CSV contents  | ðŸš§     | Check headers, format, and required fields                                     |
| &nbsp;&nbsp;&nbsp;â€¢ Load data from CSV     | ðŸš§     | Parse and ingest file contents into database                                   |
| &nbsp;&nbsp;&nbsp;â€¢ Rollback on failure    | ðŸš§     | "All or nothing" import behavior for data integrity                            |

---

### Security

#### JWT Authentication
- **Access Token** must be sent via the `Authorization: Bearer <token>` header for every requests.
- **Refresh Token** is send and receive in an `HttpOnly` cookie - this prevents access from JavaScript, helping mitigate **XSS attacks**.

#### CSRF Protection
- A **CSRF token** will be issued as a normal (non-HttpOnly) cookie.
- The requests must send this token back in the `X-CSRF-Token` header for `/auth/refresh`. This is to provent **Cross-Site Request Forgery** attacks.

#### Example using `curl` (adapt accordingly for client calls):
```bash
# authenticate and store the http cookies into txt
curl --verbose -c cookies.txt https://localhost:<port>/auth/token \
  -d 'username=<api-key>&password=<password>'

# Use stored cookie and send CSRF token in header
curl --verbose -b cookies.txt -c cookies.txt -X POST \
  https://localhost:<port>/auth/refresh \
  -H "X-CSRF-Token: <csrf-token>"
```

### Additional Notes

- **Bootstrapping** triggers scraping from [openinsider.com](http://openinsider.com) and imports data starting from `2003-01-01` (configurable).
- **Daily Sync** runs every midnight (UTC) to pull and ingest new data automatically.

## How to Run Locally

1. Clone the repository:
  ```bash
  git clone <repo-url>
  cd open-insider-trades/server 
  ```
2. Create a python virtual environment and activate it:
  ```bash
  python3 -m venv venv
  . venv/bin/activate
  ```
2. Install the dependencies:
  ```bash
  pip install -r requirements.txt
  ```
3. Set up environment variables in a `.env` file (example):
  ```
  SQLALCHEMY_DATABASE_URL=sqlite:///./<your-sqlite3>.db
  BASE_URL="http://openinsider.com/screener"  # this is where data is scraped 
  SECRET_KEY="<your-secret-key>"
  ALGORITHM="<your-jwt-algorithm>"
  ACCESS_TOKEN_EXPIRE_MINUTES=30
  MAX_WORKERS=3                               # Number of threads for data extraction
  DEFAULT_FILLING_DAYS=1                      # Custom filling date
  TRADE_DATE_FILTER=0                         # Trade date = all dates
  OUTPUT_FILE="<your-output-filename>"        # File to save the extracted data
  MAX_ROWS=5000
  SUPER_ADMIN_ID="<uuid>"
  SUPER_ADMIN_SECRET="<passcode>"
  REDIS_URL="<redis-url>"                     # Optional: the hostname/IP address of your Redis server for caching. If not defined, cache falls back to SQLite. This is for rate-limiting
  REDIS_PASSWORD="<redis-pass>"               # If Redis requires authentication, put the password here
  REDIS_PORT=6379
  REDIS_EX=600                                # Optional: Set the expiration time (in seconds) for Redis keys
  COOKIE_SECURE=True                          # Set to True if using HTTPS
  ```
4. Run the application:
  ```bash
  uvicorn main:app --reload
  ```

The API will be available at `http://127.0.0.1:8000`.

### File Structure
```
open-insider-trades/
â”œâ”€â”€ server      
â”‚  â”œâ”€â”€ main.py                    # FastAPI app entry point 
â”‚  â”œâ”€â”€ settings.py                # App settings & environment config
â”‚  â”œâ”€â”€ db.py                      # Database connection setup
â”‚  â”œâ”€â”€ models/                    # 1/ Database models      
â”‚  â”‚   â”œâ”€â”€ client.py    
â”‚  â”‚   â””â”€â”€ transaction.py        
â”‚  â”œâ”€â”€ routers/                   # 2/ API route handlers       
â”‚  â”‚   â”œâ”€â”€ admin.py  
â”‚  â”‚   â”œâ”€â”€ auth.py 
â”‚  â”‚   â”œâ”€â”€ transaction.py    
â”‚  â”‚   â””â”€â”€ utils 
â”‚  â”‚       â””â”€â”€ exceptions.py     # Custom error responses
â”‚  â”œâ”€â”€ schemas/                  # 3/ Request & Response schemas for Pydantic and OpenAPI  
â”‚  â”‚   â”œâ”€â”€ auth.py         
â”‚  â”‚   â””â”€â”€ transaction.py       
â”‚  â”œâ”€â”€ services/                 # 4/ Fetch, preprocess and prepare the data       
â”‚  â”‚   â”œâ”€â”€ transaction.py   
â”‚  â”‚   â”œâ”€â”€ ratelimiter.py     
â”‚  â”‚   â”œâ”€â”€ auth.py       
â”‚  â”‚   â””â”€â”€ utils 
â”‚  â”‚       â””â”€â”€ token.py 
â”‚  â”œâ”€â”€ requirements.txt          # Libraries Dependencies
â”‚  â”œâ”€â”€ Dockerfile                # Docker configuration
â”‚  â”œâ”€â”€ .env                      # Environment variables
â”‚  â””â”€â”€ .dockerignore             # Files ignored during Docker build
â”œâ”€â”€ .gitignore                   # Git ignored files
â”œâ”€â”€ build.sh                     # Shell script to run the app locally
â”œâ”€â”€ .env                         # Redis environment variables
â”œâ”€â”€ docker-compose.yml  
â””â”€â”€ README.md                    # Documentation

```

## References
This project follows:
1. The best practices outlined by the [Twelve-Factor App](https://12factor.net), treating logs as event streams.
2. The [Style Guide for Python Code](https://peps.python.org/pep-0008/) to ensure clean, readable, and consistent code.
3. The data validation library for Python: [Pydantic](https://docs.pydantic.dev/latest/)
